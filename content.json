{"pages":[],"posts":[{"title":"Markdown语法初入","text":"Markdown 是一种轻量级的「标记语言」，使用用特殊的 Markdown 文档处理器将 Markdown 语法翻译成预设的文档格式、标题大小等，一般用于展示时输出的是 HTML。 标题 # 后加上要设的标题就可出现效果，#与标题之间需加入空格。123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 换行两个回车表示新的段落 两个空格再加回车表示换行第二行 列表+ -表示无序列表，1. 2.表示有序列表。12345678910无序列表+ 无序列表1 + 无序列表1.1+ 无序列表2- 无序列表3- 无序列表4有序列表1. 有序列表12. 有序列表2 效果：无序列表 无序列表1 无序列表 1.1 无序列表2 无序列表3 无序列表4 有序列表 有序列表1 有序列表2 引用&gt;后接引用内容。12&gt; 引用内容1&gt; 引用内容2 效果： 引用内容1引用内容2 粗体/斜体两个* *之间的内容为斜体，两个成对** **之间的内容为粗体。12*斜体***粗体** 效果：斜体粗体 链接与图片链接：[显示文本](链接地址)1[Google](https://www.google.com) Google 图片 ：![图片显示失败时的替换文本](图片地址 &quot;图片描述文本&quot;) 1![Github](https://desktop.github.com/images/desktop-icon.svg &quot;Github&quot;) 参考Markdown：指南 by binarization Markdown 语法说明","link":"/2017/12/16/Markdown语法初入/"},{"title":"OPTICS算法基础","text":"在前面介绍的DBSCAN算法中，有两个初始参数E（邻域半径）和minPts(E邻域最小点数)需要用户手动设置输入，并且聚类的类簇结果对这两个参数的取值非常敏感，不同的取值将产生不同的聚类结果，其实这也是大多数其他需要初始化参数聚类算法的弊端。 为了克服DBSCAN算法这一缺点，提出了OPTICS算法（Ordering Points to identify the clustering structure）。OPTICS并不显示的产生结果类簇，而是为聚类分析生成一个增广的簇排序（比如，以可达距离为纵轴，样本点输出次序为横轴的坐标图），这个排序代表了各样本点基于密度的聚类结构。它包含的信息等价于从一个广泛的参数设置所获得的基于密度的聚类，换句话说，从这个排序中可以得到基于任何参数E和minPts的DBSCAN算法的聚类结果。 定义OPTICS算法的基础有两点， 参数（半径，最少点数）： 一个是输入的参数，包括:半径ε，和最少点数MinPts。 定义（核心点，核心距离，可达距离，直接密度可达）： 另一个是相关概念的定义：核心点的定义，如果一个点的半径内包含点的数量不少于最少点数，则该点为核心点，数学描述即 Nε(P)&gt;=MinPts 在这个基础上可以引出核心距离的定义，即对于核心点，距离其第MinPtsth近的点与之的距离 coreDist(P)={UNDIFED, MinPtsth Distance in N(P),if N(P)&lt;=MinPtselse 可达距离，对于核心点P，O到P的可达距离定义为O到P的距离或者P的核心距离，即公式 reachDist(O,P)={UNDIFED, max(coreDist(P), dist(O,P)),if N(P)&lt;=MinPtselse O到P直接密度可达，即P为核心点，且P到O的距离小于半径。 算法OPTICS算法的难点在于维护核心点的直接可达点的有序列表。算法的计算过程如下： 输入：数据样本D，初始化所有点的可达距离和核心距离为MAX，半径ε，和最少点数MinPts。 1、建立两个队列，有序队列（核心点及该核心点的直接密度可达点），结果队列（存储样本输出及处理次序） 2、如果D中数据全部处理完，则算法结束，否则从D中选择一个未处理且未核心对象的点，将该核心点放入结果队列，该核心点的直接密度可达点放入有序队列，直接密度可达点并按可达距离升序排列； 3、如果有序序列为空，则回到步骤2，否则从有序队列中取出第一个点； 3.1 判断该点是否为核心点，不是则回到步骤3，是的话则将该点存入结果队列，如果该点不在结果队列； 3.2 该点是核心点的话，找到其所有直接密度可达点，并将这些点放入有序队列，且将有序队列中的点按照可达距离重新排序，如果该点已经在有序队列中且新的可达距离较小，则更新该点的可达距离。 3.3 重复步骤3，直至有序队列为空。 4、算法结束。 输出结果给定半径ε，和最少点数MinPts，就可以输出所有的聚类。 计算过程为： 给定结果队列 1、从结果队列中按顺序取出点，如果该点的可达距离不大于给定半径ε，则该点属于当前类别，否则至步骤2； 2、如果该点的核心距离大于给定半径ε，则该点为噪声，可以忽略，否则该点属于新的聚类，跳至步骤1； 3、结果队列遍历结束，则算法结束。","link":"/2018/01/06/OPTICS算法基础/"},{"title":"Numpy and Matplotlib Tutorial","text":"NumpyNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. ArraysA numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of intergers giving the size of the array along each dimension. Initailize numpy arrays from nested Python lists, and access elements using square brackets: 123456789101112import numpy as npa = np.array([1, 2, 3]) # Create a rank 1 arrayprint(type(a)) # Prints \"&lt;class 'numpy.ndarray'&gt;\"print(a.shape) # Prints \"(3,)\"print(a[0], a[1], a[2]) # Prints \"1 2 3\"a[0] = 5 # Change an element of the arrayprint(a) # Prints \"[5, 2, 3]\"b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 arrayprint(b.shape) # Prints \"(2, 3)\"print(b[0, 0], b[0, 1], b[1, 0]) # Prints \"1 2 4\" Numpy also provides many functions to create arrays: 1234567891011121314151617181920import numpy as npa = np.zeros((2,2)) # Create an array of all zerosprint(a) # Prints \"[[ 0. 0.] # [ 0. 0.]]\"b = np.ones((1,2)) # Create an array of all onesprint(b) # Prints \"[[ 1. 1.]]\"c = np.full((2,2), 7) # Create a constant arrayprint(c) # Prints \"[[ 7. 7.] # [ 7. 7.]]\"d = np.eye(2) # Create a 2x2 identity matrixprint(d) # Prints \"[[ 1. 0.] # [ 0. 1.]]\"e = np.random.random((2,2)) # Create an array filled with random valuesprint(e) # Might print \"[[ 0.91940167 0.08143941] # [ 0.68744134 0.87236687]]\" Array indexingNumpy offers several ways to index into arrays. Slicing: Similaer to Python lists, numpy arrays can be sliced. Since arrays may be multidimensional, you must specify a slice for each dimension of the array: 12345678910111213141516171819import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Use slicing to pull out the subarray consisting of the first 2 rows# and columns 1 and 2; b is the following array of shape (2, 2):# [[2 3]# [6 7]]b = a[:2, 1:3]# A slice of an array is a view into the same data, so modifying it# will modify the original array.print(a[0, 1]) # Prints \"2\"b[0, 0] = 77 # b[0, 0] is the same piece of data as a[0, 1]print(a[0, 1]) # Prints \"77\" You can also mix integer indexing with slice indexing. However, doing so will yield an array of lower rank than the original array： 123456789101112131415161718192021222324import numpy as np# Create the following rank 2 array with shape (3, 4)# [[ 1 2 3 4]# [ 5 6 7 8]# [ 9 10 11 12]]a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])# Two ways of accessing the data in the middle row of the array.# Mixing integer indexing with slices yields an array of lower rank,# while using only slices yields an array of the same rank as the# original array:row_r1 = a[1, :] # Rank 1 view of the second row of arow_r2 = a[1:2, :] # Rank 2 view of the second row of aprint(row_r1, row_r1.shape) # Prints \"[5 6 7 8] (4,)\"print(row_r2, row_r2.shape) # Prints \"[[5 6 7 8]] (1, 4)\"# We can make the same distinction when accessing columns of an array:col_r1 = a[:, 1]col_r2 = a[:, 1:2]print(col_r1, col_r1.shape) # Prints \"[ 2 6 10] (3,)\"print(col_r2, col_r2.shape) # Prints \"[[ 2] # [ 6] # [10]] (3, 1)\" Integer array indexing:When you index into numpy arrays using slicing, the resulting array view will always be a subarray of the original array. In contrast, integer array indexing allows you to construct arbitrary arrays using the data from another array. 1234567891011121314151617import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])# An example of integer array indexing.# The returned array will have shape (3,) andprint(a[[0, 1, 2], [0, 1, 0]]) # Prints \"[1 4 5]\"# The above example of integer array indexing is equivalent to this:print(np.array([a[0, 0], a[1, 1], a[2, 0]])) # Prints \"[1 4 5]\"# When using integer array indexing, you can reuse the same# element from the source array:print(a[[0, 0], [1, 1]]) # Prints \"[2 2]\"# Equivalent to the previous integer array indexing exampleprint(np.array([a[0, 1], a[0, 1]])) # Prints \"[2 2]\" One usful trick with integer array indexing is selecting or mutating one element from each row of a matrix: 1234567891011121314151617181920212223import numpy as np# Create a new array from which we will select elementsa = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])print(a) # prints \"array([[ 1, 2, 3], # [ 4, 5, 6], # [ 7, 8, 9], # [10, 11, 12]])\"# Create an array of indicesb = np.array([0, 2, 0, 1])# Select one element from each row of a using the indices in bprint(a[np.arange(4), b]) # Prints \"[ 1 6 7 11]\"# Mutate one element from each row of a using the indices in ba[np.arange(4), b] += 10print(a) # prints \"array([[11, 2, 3], # [ 4, 5, 16], # [17, 8, 9], # [10, 21, 12]]) Boolean array indexing: Boolean array indexing lets you pick out arbitary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. 1234567891011121314151617181920import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])bool_idx = (a &gt; 2) # Find the elements of a that are bigger than 2; # this returns a numpy array of Booleans of the same # shape as a, where each slot of bool_idx tells # whether that element of a is &gt; 2.print(bool_idx) # Prints \"[[False False] # [ True True] # [ True True]]\"# We use boolean array indexing to construct a rank 1 array# consisting of the elements of a corresponding to the True values# of bool_idxprint(a[bool_idx]) # Prints \"[3 4 5 6]\"# We can do all of the above in a single concise statement:print(a[a &gt; 2]) # Prints \"[3 4 5 6]\" DatatypesEvery numpy array is a grid of elements of the same type. Numpy provides a large set of numeric datatypes that you can use to construct arrays. Numpy tries to guess a datatype when you create an array, but functions that construct arrays usually also include an optional argument to explictly specify the datatype. 12345678910import numpy as npx = np.array([1, 2]) # Let numpy choose the datatypeprint(x.dtype) # Prints \"int64\"x = np.array([1.0, 2.0]) # Let numpy choose the datatypeprint(x.dtype) # Prints \"float64\"x = np.array([1, 2], dtype=np.int64) # Force a particular datatypeprint(x.dtype) # Prints \"int64\" Array mathBasic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module: 123456789101112131415161718192021222324252627282930313233import numpy as npx = np.array([[1,2],[3,4]], dtype=np.float64)y = np.array([[5,6],[7,8]], dtype=np.float64)# Elementwise sum; both produce the array# [[ 6.0 8.0]# [10.0 12.0]]print(x + y)print(np.add(x, y))# Elementwise difference; both produce the array# [[-4.0 -4.0]# [-4.0 -4.0]]print(x - y)print(np.subtract(x, y))# Elementwise product; both produce the array# [[ 5.0 12.0]# [21.0 32.0]]print(x * y)print(np.multiply(x, y))# Elementwise division; both produce the array# [[ 0.2 0.33333333]# [ 0.42857143 0.5 ]]print(x / y)print(np.divide(x, y))# Elementwise square root; produces the array# [[ 1. 1.41421356]# [ 1.73205081 2. ]]print(np.sqrt(x)) Note that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matices. dot is available both as a function in the numpy module and as an instance method of array objects: 123456789101112131415161718192021import numpy as npx = np.array([[1,2],[3,4]])y = np.array([[5,6],[7,8]])v = np.array([9,10])w = np.array([11, 12])# Inner product of vectors; both produce 219print(v.dot(w)) # An instance methodprint(np.dot(v, w)) # A function in the numpy module# Matrix / vector product; both produce the rank 1 array [29 67]print(x.dot(v))print(np.dot(x, v))# Matrix / matrix product; both produce the rank 2 array# [[19 22]# [43 50]]print(x.dot(y))print(np.dot(x, y)) Numpy provides many useful functions for performing computations on arrays; one of the most useful if sum: 1234567import numpy as npx = np.array([[1,2],[3,4]])print(np.sum(x)) # Compute sum of all elements; prints \"10\"print(np.sum(x, axis=0)) # Compute sum of each column; prints \"[4 6]\"print(np.sum(x, axis=1)) # Compute sum of each row; prints \"[3 7]\" Apart from computing mathematical functions using arrays, we frequently need to reshape or otherwise manipulate data in arrays. The simplest example of this type of operation is transposing a matrix; to transpose a matrix, simply use the T attribute of an array object: 123456789101112import numpy as npx = np.array([[1,2], [3,4]])print(x) # Prints \"[[1 2] # [3 4]]\"print(x.T) # Prints \"[[1 3] # [2 4]]\"# Note that taking the transpose of a rank 1 array does nothing:v = np.array([1,2,3])print(v) # Prints \"[1 2 3]\"print(v.T) # Prints \"[1 2 3]\" BroadcastingBroadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array. For example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this: 123456789101112131415161718import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = np.empty_like(x) # Create an empty matrix with the same shape as x# Add the vector v to each row of the matrix x with an explicit loopfor i in range(4): y[i, :] = x[i, :] + v# Now y is the following# [[ 2 2 4]# [ 5 5 7]# [ 8 8 10]# [11 11 13]]print(y) This works; however when the matrix x is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the matrix x is equivalent to forming a matrix vv by stacking multiple copies of v vertically, then performing elementwise summation of x and vv. We could implement this approach like this: 12345678910111213141516import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])vv = np.tile(v, (4, 1)) # Stack 4 copies of v on top of each otherprint(vv) # Prints \"[[1 0 1] # [1 0 1] # [1 0 1] # [1 0 1]]\"y = x + vv # Add x and vv elementwiseprint(y) # Prints \"[[ 2 2 4 # [ 5 5 7] # [ 8 8 10] # [11 11 13]]\" Numpy broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting: 1234567891011 import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint(y) # Prints \"[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]\" The line y = x + x works even though x has shape (4, 3) and v has shape (3, ) due to broadcasting; this line works as if v actually had shape(4, 3), where each row was a copy of v, and the sum was performed elementwise. Broadcasting two arrays together follows these rules: If the arrays do not have the same rank, prepend the shape of the lower rank array with 1s util both shapes have the same length. The two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension. The arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of the shapes of the two input arrays. In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension. Functions that support broadcasting are known as universal functions. Here are some applications of brodcasting: 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:# [[ 4 5]# [ 8 10]# [12 15]]print(np.reshape(v, (3, 1)) * w)# Add a vector to each row of a matrixx = np.array([[1,2,3], [4,5,6]])# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),# giving the following matrix:# [[2 4 6]# [5 7 9]]print(x + v)# Add a vector to each column of a matrix# x has shape (2, 3) and w has shape (2,).# If we transpose x then it has shape (3, 2) and can be broadcast# against w to yield a result of shape (3, 2); transposing this result# yields the final result of shape (2, 3) which is the matrix x with# the vector w added to each column. Gives the following matrix:# [[ 5 6 7]# [ 9 10 11]]print((x.T + w).T)# Another solution is to reshape w to be a column vector of shape (2, 1);# we can then broadcast it directly against x to produce the same# output.print(x + np.reshape(w, (2, 1)))# Multiply a matrix by a constant:# x has shape (2, 3). Numpy treats scalars as arrays of shape ();# these can be broadcast together to shape (2, 3), producing the# following array:# [[ 2 4 6]# [ 8 10 12]]print(x * 2) Numpy DocumentationThis brief overview has touched on many of the important things need to know about numpy, but is far from complete. Check out the numpy [numpy reference][http://docs.scipy.org/doc/numpy/reference/] to find out much more. MatplotlibMatplotlib is a plotting library. In this section give a brief introduction to the matplotlib.pyplot module, which provides a plotting system similar to that of MATLAB. PlottingThe most important function in matplotlib is plot, which allows you to plot 2D data. Here is a simple example: 12345678910import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on a sine curvex = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)# Plot the points using matplotlibplt.plot(x, y)plt.show() # You must call plt.show() to make graphics appear. Runnig this code produces the following plot: With just a little bit of extra work we can easily plot multiple lines at once, and add a title, legend, and axis labels: 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Plot the points using matplotlibplt.plot(x, y_sin)plt.plot(x, y_cos)plt.xlabel('x axis label')plt.ylabel('y axis label')plt.title('Sine and Cosine')plt.legend(['Sine', 'Cosine'])plt.show() SubplotsYou can plot different things in the same figure using the subplot function. Here is an example: 1234567891011121314151617181920212223mport numpy as npimport matplotlib.pyplot as plt# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Set up a subplot grid that has height 2 and width 1,# and set the first such subplot as active.plt.subplot(2, 1, 1)# Make the first plotplt.plot(x, y_sin)plt.title('Sine')# Set the second subplot as active, and make the second plot.plt.subplot(2, 1, 2)plt.plot(x, y_cos)plt.title('Cosine')# Show the figure.plt.show() ImagesYou can use the imshow function to show images. Here is an example: 12345678910111213141516171819import numpy as npfrom scipy.misc import imread, imresizeimport matplotlib.pyplot as pltimg = imread('assets/cat.jpg')img_tinted = img * [1, 0.95, 0.9]# Show the original imageplt.subplot(1, 2, 1)plt.imshow(img)# Show the tinted imageplt.subplot(1, 2, 2)# A slight gotcha with imshow is that it might give strange results# if presented with data that is not uint8. To work around this, we# explicitly cast the image to uint8 before displaying it.plt.imshow(np.uint8(img_tinted))plt.show() Reference[Python numpy tutorial][http://cs231n.github.io/python-numpy-tutorial/]","link":"/2018/05/21/Python Numpy Tutorial/"},{"title":"Restricted Boltzmann Machine","text":"受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）是G.Hinton教授的一宝。Hinton教授是深度学习的开山鼻祖，也正是他在2006年的关于深度信念网络DBN的工作，以及逐层预训练的训练方法，开启了深度学习的序章。其中，DBN中在层间的预训练就采用了RBM算法模型。RBM是一种无向图模型，也是一种神经网络模型。 RBM具有两层：可见层（V层），以及隐藏层（H层）。可以看到，两层神经元之间都是全连接的，但是每一层各自的神经元之间并没有连接，也就是说，RBM的图结构是一种二分图（bipartite graph）。正是这个特点，才叫受限玻尔兹曼机，玻尔兹曼机是允许同一层之间的神经元相连的。RBM其实是一种简化了的BM模型。 还有一个特点，RBM中的神经元都是二值化的，也就是说只有激活和不激活两种状态，也就是0或者1；可见层和隐藏层之间的边的权重可以用$W$来表示，$W$是一个$|V|×|H|$大小的实数矩阵。算法难点主要就是对$W$求导（当然还有bias参数），用于梯度下降的更新；但是因为$V$和$H$都是二值化的，没有连续的可导函数去计算，实际中采用的sampling的方法来计算，这里面就可以用比如Gibbs sampling的方法，当然，Hinton提出了对比散度DC方法，比Gibbs方法更快，已经成为求解RBM的标准解法。 RBM模型结构因为RBM隐层和可见层是全连接的，为了描述清楚与容易理解，把每一层的神经元展平即可，见下图[7]，本文后面所有的推导都采用下图中的标记来表示。 再重提一下，经典的RBM模型中的神经元都是binary的，也就是说上面图中的神经元取值都是$(0,1) $的。实际上RBM也可以做实数性的model，不过这一块可以先放一放，先来看binary的基本model。 RBM能量函数RBM是一个能量模型（Energy based model, EBM），是从物理学能量模型中演变而来；能量模型需要做的事情就是先定义一个合适的能量函数，然后基于这个能量函数得到变量的概率分布，最后基于概率分布去求解一个目标函数（如最大似然）。RBM的过程如下： 我们现在有的变量是$(v,h)$，包括隐层和可见层神经元；参数包括$θ=(W,a,b)$。能量函数定义： $$ E_θ(v,h)=−∑_{i=1}^{n_v}a_iv_i−∑_{j=1}^{n_h}b_jh_j−∑_{i=1}^{n_v}∑_{j=1}^{n_h}h_jw_{j,i}v_i$$ 如果写成向量/矩阵的形式，则为： $$E_θ(v,h)=−a^Tv−b^Th−h^TWv$$ 那么，可以得到变量$(v,h)$的联合概率分布是： $$P_θ(v,h)=\\frac {1}{Z_θ}e^{−E_θ(v,h)}$$ 其中，$Z_θ$称为归一化因子，作用是使得概率之和（或者积分）为1，形式为： $$Z_θ=∑_{v,h}e^{−E_θ(v,h)}$$ 在上面所有算式中，下标$θ$都表示在参数$θ=(W,a,b)$下的表达，为了书写的简洁性，本文余下部分如果没有特殊指定说明，就省略下标$θ$了，但含义不变。 当我们有了联合概率分布，如果想求观察数据（可见层）的概率分布$P(v)$，则求边缘分布： $$P(v)=∑_hP(v,h)=\\frac1Z∑_he^{−E(v,h)}$$ 相对应的，如果想求隐层单元的概率分布$P(h)$，则求边缘分布： $$P(h)=∑_vP(v,h)=\\frac1Z∑_ve^{−E(v,h)}$$ 当然，我们不太可能直接计算$Z$，因为$Z$的求和中存在指数项种可能——$2^{n_v+n_h}$种取值。接下来考虑条件概率，即可见层神经元状态给定时，（任意）隐藏层神经元状态为1的概率，即$P(h_k=1|v)$。类似的也可以求$P(v_k=1|h)$，方法也是差不多的，下面就只对$P(h_k=1|v)$进行描述。我们可以推导出： $$P(h_k=1|v)=sigmoid(b_k+∑_{j=1}^{n_v}w_{kj}v_j)$$ 以及 $$P(v_k=1|h)=sigmoid(b_k+∑_{j=1}^{n_v}w_{kj}h_j)$$ 可以直接知道结果即可，证明可以跳过。这里我们可以看到，sigmoid是一种激励函数，因此才把RBM也叫做一种神经网络模型。 证：以下记$h_{−k}$表示隐藏层神经元k以外的神经元。 $$P(h_k=1|v)=P(h_k=1|h_{−k},v)$$ $$=\\frac {P(h_k=1,h_{−k},v)}{P(h_{−k},v))}$$ $$=\\frac {P(h_k=1,h_{−k},v)}{P(h_k=1,h_{−k},v))+P(h_k=0,h_{−k},v))}$$ $$=\\frac{\\frac1Ze^{−E(h_k=1,h_{−k},v)}}{\\frac1Ze^{−E(h_k=1,h_{−k},v))}+\\frac 1Ze^{−E(h_k=0,h_{−k},v))}}$$ $$=\\frac1{1+e^{−E(h_k=0,h_{−k},v))+E(h_k=1,h_{−k},v)}}$$ $$=\\frac 1{1+e^{−(b_k+∑^{n_v}_{j=1}w_{kj}v_j)}}$$ $$=sigmoid(b_k+∑_{j=1}^{n_v}w_{kj}v_j)$$ 因为假设同层神经元之间相互独立，所以有： $$P(h|v)=∏_{j=1}^{n_h}P(h_j|v)$$ $$P(v|h)=∏_{i=1}^{n_v}P(v_i|h)$$ RBM目标函数假设给定的训练集合是$S=\\{v^i\\}$，总数是$n_s$，其中每个样本表示为$v^i=(v^i_1,v^i_2,…,v^i_{n_v})$，且都是独立同分布i.i.d的。RBM采用最大似然估计，即最大化 $$\\ln L_S=\\ln ∏_{i=1}^{n_s}P(v^i)=∑_{i=1}^{n_s}\\ln P(v^i)$$ 参数表示为$θ=(W,a,b)$，因此统一的参数更新表达式为： $$θ=θ+η\\frac{∂\\ln LS}{∂θ}$$ 其中，$η$表示学习速率。因此，很明显，只要我们可以求解出参数的梯度，我们就可以求解RMB模型了。我们先考虑任意单个训练样本$(v^0)$的情况，即 $$L_S=\\ln ⁡P(v^0)=\\ln ⁡(\\frac1Z∑_he^{−E(v^0,h)})$$ $$=\\ln⁡∑_he^{−E(v^0,h)}−\\ln⁡∑_{v,h}e^{−E(v,h)}$$ 其中$v$表示任意的训练样本，而$v^0$则表示一个特定的样本。 $$\\frac {∂L_S}{∂θ}=\\frac {∂\\ln P(v^0)}{∂θ}$$ $$=\\frac ∂{∂θ}(\\ln ∑_he^{−E(v^0,h)})−\\frac ∂{∂θ}(\\ln∑_{v,h}e^{−E(v,h)})$$ $$=−\\frac1{∑_he^{−E(v^0,h)}}∑_he^{−E(v^0,h)}\\frac {∂E(v^0,h)}{∂θ}+\\frac 1{∑_{v,h}e^{−E(v,h)}}∑_{v,h}e^{−E(v,h)}\\frac {∂E(v,h)}{∂θ}$$ $$=−∑_hP(h|v^0)\\frac {∂E(v^0,h)}{∂θ}+∑_{v,h}P(h,v)\\frac {∂E(v,h)}{∂θ}$$ （其中第3个等式左边内条件概率$P(h|v^0)$，因为$\\frac {e^−E(v^0,h)}{∑_he^{−E(v^0,h)}}=\\frac {e^{−E(v^0,h)}/Z}{∑_he^{−E(v^0,h)}/Z}=\\frac{P(v^0,h)}{P(v^0)}=P(h|v^0)$） 上面式子的两个部分的含义是期望——左边是梯度$\\frac{∂E(v0,h)}{∂θ}$在条件概率分布$P(h|v^0)$下的期望；右边是梯度$\\frac {∂E(v,h)}{∂θ}$在联合概率分布$P(h,v)$下的期望。要求前面的条件概率是比较容易一些的，而要求后面的联合概率分布是非常困难的，因为它包含了归一化因子$Z$（对所有可能的取值求和，连续的情况下是积分），因此我们采用一些随机采样来近似求解。把上面式子再推导一步，可以得到， $$\\frac{∂L_S}{∂θ}=−∑_hP(h|v^0)\\frac{∂E(v^0,h)}{∂θ}+∑_vP(v)∑_hP(h|v)\\frac{∂E(v,h)}{∂θ}$$ 因此，我们重点就是需要就算$∑_hP(h|v)\\frac{∂E(v,h)}{∂θ}$，特别的，针对参数$W$,$a$,$b$来说，有 $$∑_hP(h|v)\\frac {∂E(v,h)}{∂w_{ij}}=−∑_hP(h|v)h_iv_j$$ $$=−∑_hP(h_i|v)P(h_{−i}|v)h_iv_j$$ $$=−∑_{h_i}P(h_i|v)∑_{h_−i}P(h_{−i}|v)h_iv_j$$ $$=−∑_{h_i}P(h_i|v)h_iv_j$$ $$=−(P(h_i=1|v)⋅1⋅v_j+P(h_i=0|v)⋅0⋅v_j)$$ $$=−P(h_i=1|v)v_j$$ 类似的，我们可以很容易得到： $$\\sum _hP(h|v)\\frac{∂E(v,h)}{∂a_i}=−v_i$$ $$∑_hP(h|v)\\frac{∂E(v,h)}{∂b_j}=−P(h_i=1|v)$$ 于是，我们很容易得到， $$\\frac {∂\\ln P(v^0)}{∂w_{ij}}=−∑_hP(h|v^0)\\frac {∂E(v^0,h)}{∂w_{ij}}+∑_vP(v)∑_hP(h|v)\\frac {∂E(v,h)}{∂w_{ij}}$$ $$=P(h_i=1|v^0)v^0_j−∑_vP(v)P(h_i=1|v)v_j$$ $$\\frac{∂\\ln P(v^0)}{∂a_i}=v^0_i−∑_vP(v)v_i$$ $$\\frac {∂\\ln P(v^0)}{∂b_i}=P(h_i=1|v^0)−∑_vP(v)P(h_i=1|v)$$ 上面求出了一个样本的梯度，对于$n_s$个样本有 $$\\frac {∂L_S}{∂w_{ij}}=\\sum _{m=1}^{n_s}[P(h_i=1|v^m)v^m_j−∑_vP(v)P(h_i=1|v)v_j]$$ $$\\frac{∂L_S}{∂a_i}=\\sum _{m=1}^{n_s}[v^m_i−∑_vP(v)v_i]$$ $$\\frac {∂L_S}{∂b_i}=\\sum _{m=1}^{n_s}[P(h_i=1|v^m)−∑_vP(v)P(h_i=1|v)]$$ 到这里就比较明确了，主要就是要求出上面三个梯度；但是因为不好直接求概率分布$P(v)$，前面分析过，计算复杂度非常大，因此采用一些随机采样的方法来得到近似的解。看这三个梯度的第二项实际上都是求期望，而我们知道，样本的均值是随机变量期望的无偏估计。 Gibbs Sampling吉布斯采样（Gibbs sampling），是MCMC方法的一种。总的来说，Gibbs采样可以从一个复杂概率分布$P(X)$下生成数据，只要我们知道它每一个分量的相对于其他分量的条件概率$P(X_k|X_{−k})$，就可以对其进行采样。而RBM模型的特殊性，隐藏层神经元的状态只受可见层影响（反之亦然），而且同一层神经元之间是相互独立的，那么就可以根据如下方法依次采样： 也就是说$hi$是以概率$P(h_i|v_0)$为1，其他的都类似。这样当我们迭代足够次以后，我们就可以得到满足联合概率分布$P(v,h)$下的样本$(v,h)$，其中样本$(v)$可以近似认为是$P(v)$下的样本，下图也说明了这个迭代采样的过程：有了样本$(v)$就可以求出上面写到的三个梯度$(\\frac {∂L_S}{∂w_{ij}},\\frac {∂L_S}{∂a_i},\\frac{∂L_S}{∂b_i})$了，用梯度上升就可以对参数进行更新了。（实际中，可以在k次迭代以后，得到样本集合${v}$，比如迭代100次取后面一半，带入上面梯度公式的后半部分计算平均值。） 看起来很简单是不是？但是问题是，每一次gibbs采样过程都需要反复迭代很多次以保证马尔科夫链收敛，而这只是一次梯度更新，多次梯度更新需要反复使用gibbs采样，使得算法运行效率非常低。为了加速RBM的训练过程，Hinton等人提出了对比散度（Contrastive Divergence）方法，大大加快了RBM的训练速度。 Contrastive Divergence我们希望得到$P(v)$分布下的样本，而我们有训练样本，可以认为训练样本就是服从$P(v)$的。因此，就不需要从随机的状态开始gibbs采样，而从训练样本开始。 CD算法大概思路是这样的，从样本集任意一个样本$v^0$开始，经过k次Gibbs采样（实际中k=1往往就足够了），即每一步是： $$h^{t−1}∼P(h|v^{t−1)}$$ $$v^t∼P(v|h^{t−1})$$ 得到样本{v^k}然后对应于三个单样本的梯度，用$v^k$去近似： $$\\frac {∂\\ln P(v)}{∂w_{ij}}\\approx P(h_i=1|v^0)v^0_j-P(h_i=1|v^k)v^k_j$$ $$\\frac {∂\\ln P(v)}{∂a_{i}}\\approx v^0_i-v^k_i $$ $$\\frac {∂\\ln P(v)}{∂b_{i}}\\approx P(h_i =1| v^0)-P(h_i=1|v^k)$$ 上述近似的含义是说，用一个采样出来的样本来近似期望的计算。到这里，我们就可以计算$L_S$的梯度了，上面的CD-k算法是用于在一次梯度更新中计算梯度近似值的。下面给出CD-k的算法执行流程，这里小偷懒一下，就借用截图了[7]。 其中，sample_h_given_v(v,W,a,b)，做的事情是这样（sample_v_given_v(h,W,a,b)类似）：记$q_j=P(h_j|v),j=1,2,…,n_h$，产生一个[0,1]的随机数$r_j$，对每一个$h_j$，如果$r_j&lt;q_j$，则$h_j=1$，否则$h_j=0$。 OK， 有了CD-k算法，我们也可以总结RMB整个算法了[7]， 参考资料Bin的专栏 张春霞，受限波兹曼机简介","link":"/2018/05/22/Restricted Boltzmann Machine/"},{"title":"Beam Search算法过程","text":"在sequence2sequence模型中，beam search的方法只用在测试的情况，因为在训练过程中，每一个decoder的输出是有正确答案的，也就不需要beam search去加大输出的准确率。 假设现在我们用机器翻译作为例子来说明， 我们需要翻译中文“我是中国人”—&gt;英文“I am Chinese” 假设我们的词表大小只有三个单词就是I am Chinese。那么如果我们的beam size为2的话，我们现在来解释, 如下图所示，我们在decoder的过程中，有了beam search方法后，在第一次的输出，我们选取概率最大的”I”和”am”两个单词，而不是只挑选一个概率最大的单词。 然后接下来我们要做的就是，把“I”单词作为下一个decoder的输入算一遍得到y2的输出概率分布，把“am”单词作为下一个decoder的输入算一遍也得到y2的输出概率分布。 比如将“I”单词作为下一个decoder的输入算一遍得到y2的输出概率分布如下： 比如将“am”单词作为下一个decoder的输入算一遍得到y2的输出概率分布如下： 那么此时我们由于我们的beam size为2，也就是我们只能保留概率最大的两个序列，此时我们可以计算所有的序列概率： “I I” = 0.40.3 “I am” = 0.40.6 “I Chinese” = 0.4*0.1 “am I” = 0.50.3 “am am” = 0.50.3 “am Chinese” = 0.5*0.4 我们很容易得出俩个最大概率的序列为 “I am”和“am Chinese”，然后后面会不断重复这个过程，直到遇到结束符为止。 最终输出2个得分最高的序列。 这就是seq2seq中的beam search算法过程，但是可能有些同学有一个疑问，就是但i-1时刻选择的单词不同的时候，下一时刻的输出概率分布为什么会改变？ 这是由于解码的过程中，第i时刻的模型的输入，包括了第i-1时刻模型的输出，那么很自然在第i-1时刻模型的输出不同的时候，就会导致下一时刻模型的输出概率分布会不同，因为第 i-1时刻的输出作为参数影响了后一时刻模型的学习。 如下图用了一个slides的法语翻译为英文的例子，可以更容易理解上面的解释。 致谢： 凌志、刘洋、皓宇、萧瑟师兄 参考：谁能解释下seq2seq中的beam search算法过程?","link":"/2018/04/16/seq2seq中的beam search算法过程/"},{"title":"梯度下降法","text":"梯度下降法（Gradient descent）是一个一阶最优化算法，通常也称最速下降法。要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点，这个过程被称为梯度上升法。 描述梯度下降法基于以下的观察：如果实值函数 $F(x)$在点$a$处可微且有定义，那么函数$F(x)$在$a$点沿着梯度相反的方向$-\\nabla F(a)$下降最快。 因而，如果$b=a-\\gamma \\nabla F(a)$ ，对于$\\gamma &gt;0$为一个够小数值时成立，那么$F(a)\\geq F(b)$ 。 考虑到这一点，我们可以从函数$F$代码局部极小值的初始估计$x_0$出发，并考虑如下序列$x_0, x_1, x_2, \\ldots$使得$x_{n+1}=x_n - \\gamma_n\\nabla F(x_n), n\\geq 0$ ，因此可得到$F(x_0)\\ge F(x_1) \\ge F(x_2) \\ge \\cdots$ 。 如果顺利的话序列$(x_n)$收敛到期望的极值。每次的迭代步长$\\gamma$可以改变。 下面的图片示例了这一过程，这里假设$F$定义在平面上，并且函数图像是一个碗形。蓝色的曲线是等高线，即函数$F$为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。需要注意的是，某一点处的梯度方向与通过该点的等高线垂直。沿着梯度下降方向，将最终到达碗底，即函数$F$值最小的点。 梯度下降法处理一些复杂的非线性函数会出现问题，例如Rosenbrock函数$f(x, y) =(1-x)^2 + 100(y-x^2)^2$ 。其最小值在$(x,y)=(1,1)$处，数值为$f(x,y)=0$。但是此函数具有狭窄弯曲的山谷，最小值$(x,y)=(1,1)$就在这些山谷之中，并且谷底很平。优化过程是之字形的向极小值点靠近，速度非常缓慢。利用梯度下降法求解需要很多次的迭代。 梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的缺点包括： 靠近极小值时速度减慢。 直线搜索可能会产生一些问题。 可能会“之字型”地下降。 Batch Gradient Descent在优化目标函数的时候，Batch Gradient Descent(BGD)是先计算整个数据集上的梯度，然后再进行更新操作。对于参数$\\theta$来说，每更新一次其中的某一位权重$\\theta_j$，BGD都需要遍历整个数据集。 对于目标函数$h_θ(x)$用公式来表示就是$θ_j:=θ_j+\\alpha \\sum_{i=1}^m(y^i−h_θ(x^i))x_j^i$ 其中的$(y^i−h_θ(x^i))x^i_j$其实就是对于训练样例$x^i$的$j$属性的梯度，$m$是训练集的大小。从上式中可以看到，BGD是对整个数据集进行扫描，然后计算整体梯度($\\sum$求和过程)，再进行更新。其实，这才是真正的梯度. BGD的优点在于对于凸问题，它是能够保证收敛到全局最优点的。而缺点就是，计算量很大，计算每一位的权重都要遍历整个数据集，这代价未免太大了，计算量是无法接受的。随之而来的另外一个缺点就是BGD是无法进行online训练的，它必须要知道全部的训练集的情况下才能进行训练，这对于一些线上系统也是一个问题。 Stochastic Gradient DescentSGD是对BGD的一个改进方案，改变之处在更新时不需要遍历整个数据集，而是每一个实例都进行更新。具体公式$θ_j:=θ_j+α(y^i−h_θ(x^i))x^i_j$ 比较上式和BGD的公式，我们可以发现区别就在省略了求和过程$\\sum$，也就是说更新权重的时候，不需要计算整体的梯度，而是仅仅依靠当前实例的梯度进行更新。 如此改变之后，速度明显提高了很多，但是这也是有风险的。由于进行频繁的梯度更新，很有可能直接跳过了最优点。因此，SGD实际上是无法保证收敛到全局最优点的，而且不是那么的稳定。 Mini-Batch Gradient Descent而Mini-Batch是对上述两种策略的一种中和，它的基本思想就是从整个训练集上选取一个子集，对这个自己进行BGD的更新。具体公式可以表示为:$θ_j:=θ_j+α\\sum_{i=1}^n(y^i−h_θ(x^i))x^i_j$ 与BGD的式子相比，会发现唯一的区别在于求和时的项数不一样，此处的$n$不再是训练集的大小，而是一个小于或等于$m$的数，通常范围在于32-256（一般取2的整数幂）。 简单来说，先把大小为$m$的训练集分为大小为$n$的$m_n$个子集（一般最后一个batch的样本个数小于$n$），每次读入一个子集进行梯度计算，更新权重。 相比SGD来说，它更加稳定；相比BGD来说，它计算量较小。 总结 BGD SGD mini-Batch 训练集 固定 固定 固定 单次迭代样本数 整个训练集 固定 训练集的子集 算法复杂度 高 低 一般 收敛性 稳定 不稳定 较稳定 参考梯度下降法wiki 三种梯度下降法 最优化方法：梯度下降","link":"/2018/01/28/梯度下降法/"},{"title":"理解ResNet、Inception与Xception","text":"转载自机器之心，原文作者Joyce Xu 过去几年来，计算机视觉领域中深度学习的诸多进展都可以归结到几种神经网络架构。除开其中的所有数学内容、代码和实现细节，我想探索一个简单的问题：这些模型的工作方式和原因是什么？ 在本文写作时，Keras 已经将这 6 种预训练模型集成到了库中： VGG16 VGG19 ResNet50 Inception v3 Xception MobileNet VGG 网络以及从 2012 年以来的 AlexNet 都遵循现在的基本卷积网络的原型布局：一系列卷积层、最大池化层和激活层，最后还有一些全连接的分类层。MobileNet 本质上是为移动应用优化后的 Xception 架构的流线型（streamline）版本。但剩下的三个却真正重新定义了我们看待神经网络的方式。 本文的剩余部分将聚焦于 ResNet、Inception 和 Xception 架构背后的直观原理，并将解释为什么它们成为了计算机视觉领域后续许多成果的构建模块。 ResNetResNet 诞生于一个美丽而简单的观察：为什么非常深度的网络在增加更多层时会表现得更差？ 直觉上推测，更深度的网络不会比更浅度的同类型网络表现更差吧，至少在训练时间上是这样（当不存在过拟合的风险时）。让我们进行一个思想实验，假设我们已经构建了一个 n 层网络，并且实现了一定准确度。那么一个 n+1 层网络至少也应该能够实现同样的准确度——只要简单复制前面 n 层，再在最后一层增加一层恒等映射就可以了。类似地，n+2、n+3 和 n+4 层的网络都可以继续增加恒等映射，然后实现同样的准确度。但是在实际情况下，这些更深度的网络基本上都会表现得更差。 ResNet 的作者将这些问题归结成了一个单一的假设：直接映射是难以学习的。而且他们提出了一种修正方法：不再学习从 x 到 H(x) 的基本映射关系，而是学习这两者之间的差异，也就是「残差（residual）」。然后，为了计算 H(x)，我们只需要将这个残差加到输入上即可。 假设残差为 F(x)=H(x)-x，那么现在我们的网络不会直接学习 H(x) 了，而是学习 F(x)+x。 这就带来了你可能已经见过的著名 ResNet（残差网络）模块： ResNet 的每一个「模块（block）」都由一系列层和一个「捷径（shortcut）」连接组成，这个「捷径」将该模块的输入和输出连接到了一起。然后在元素层面上执行「加法（add）」运算，如果输入和输出的大小不同，那就可以使用零填充或投射（通过 1×1 卷积）来得到匹配的大小。 回到我们的思想实验，这能大大简化我们对恒等层的构建。直觉上就能知道，比起从头开始学习一个恒等变换，学会使 F(x) 为 0 并使输出仍为 x 要容易得多。一般来说，ResNet 会给层一个「参考」点 x，以 x 为基础开始学习。 这一想法在实践中的效果好得让人吃惊。在此之前，深度神经网络常常会有梯度消失问题的困扰，即来自误差函数的梯度信号会在反向传播回更早的层时指数级地下降。本质上讲，在误差信号反向回到更早的层时，它们会变得非常小以至于网络无法学习。但是，因为 ResNet 的梯度信号可以直接通过捷径连接回到更早的层，所以我们一下子就可以构建 50 层、101 层、152 层甚至 1000 层以上的网络了，而且它们的表现依然良好。那时候，这在当时最佳的基础上实现了巨大的飞跃——这个 22 层的网络赢得了 ILSVRC 2014 挑战赛。 ResNet 是神经网络领域我个人最喜欢的进展之一。很多深度学习论文都是通过对数学、优化和训练过程进行调整而取得一点点微小的进步，而没有思考模型的底层任务。ResNet 则从根本上改变了我们对神经网络及其学习方式的理解。 有趣的事实： 上面提到的 1000 多层的网络是开源的！我并不推荐你尝试再训练，但如果你就是要上：https://github.com/KaimingHe/resnet-1k-layers 如果你觉得这有用而且有点跃跃欲试，我最近将 ResNet50 移植到了开源的 Clojure ML 库 Cortex 中：https://github.com/thinktopic/cortex。你可以试试，看与 Keras 比较如何！ Inception如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。Inception 的作者对训练更大型网络的计算效率尤其感兴趣。换句话说：我们怎样在不增加计算成本的前提下扩展神经网络？ Inception 最早的论文关注的是一种用于深度网络的新型构建模块，现在这一模块被称为「Inception module」。究其核心，这种模块源自两种思想见解的交汇。 第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。但是，不同类型的层会提取不同种类的信息。5×5 卷积核的输出中的信息就和 3×3 卷积核的输出不同，又不同于最大池化核的输出……在任意给定层，我们怎么知道什么样的变换能提供最「有用」的信息呢？ 见解 1：为什么不让模型选择？ Inception 模块会并行计算同一输入映射上的多个不同变换，并将它们的结果都连接到单一一个输出。换句话说，对于每一个层，Inception 都会执行 5×5 卷积变换、3×3 卷积变换和最大池化。然后该模型的下一层会决定是否以及怎样使用各个信息。 这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。 想一下，每额外增加一个过滤器，我们就必须对所有输入映射进行卷积运算以计算单个输出。如下图所示：从单个过滤器创建一个输出映射涉及到在之前一层的每个单个映射上执行计算。 假设这里有 M 个输入映射。增加一个过滤器就意味着要多卷积 M 次映射；增加 N 个过滤器就意味着要多卷积 N*M 次映射。换句话说，正如作者指出的那样：「过滤器数量的任何统一增长都会导致计算量的 4 倍增长。」我们的朴素 Inception 模块只是将过滤器的数量增加了三四倍。但从计算成本上看，这简直就是一场大灾难。 这就涉及到了见解 2：使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。 这能达到多好的效果？Inception 的第一个版本是 GoogLeNet，也就是前面提及的赢得了 ILSVRC 2014 比赛的 22 层网络。一年之后，研究者在第二篇论文中发展出了 Inception v2 和 v3，并在原始版本上实现了多种改进——其中最值得一提的是将更大的卷积重构成了连续的更小的卷积，让学习变得更轻松。比如在 v3 中，5×5 卷积被替换成了两个 连续的 3×3 卷积。 Inception 很快就变成了一种具有决定性意义的模型架构。最新的版本 Inception v4 甚至将残差连接放进了每一个模组中，创造出了一种 Inception-ResNet 混合结构。但更重要的是，Inception 展现了经过良好设计的「网中有网」架构的能力，让神经网络的表征能力又更上了一层楼。 有趣的事实： 最早的 Inception 论文确实引用了来自电影《盗梦空间（Inception）》的「我们需要更深」的互联网模因作为其名字的来源，参见：http://knowyourmeme.com/memes/we-need-to-go-deeper。这肯定是 knowyourmeme.com (http://knowyourmeme.com/) 第一次出现在一篇谷歌论文的参考文献里。 第二篇 Inception 论文（提出 v2 和 v3）是在最早的 ResNet 论文发布之后的第二天发布的。2015 年 12 月真是深度学习的好日子。 XceptionXception 表示「extreme inception」。和前面两种架构一样，它重塑了我们看待神经网络的方式——尤其是卷积网络。而且正如其名字表达的那样，它将 Inception 的原理推向了极致。 它的假设是：「跨通道的相关性和空间相关性是完全可分离的，最好不要联合映射它们。」 这是什么意思？在传统的卷积网络中，卷积层会同时寻找跨空间和跨深度的相关性。让我们再看一下标准的卷积层： 在上图中，过滤器同时考虑了一个空间维度（每个 2×2 的彩色方块）和一个跨通道或「深度」维度（4 个方块的堆叠）。在输入图像的输入层，这就相当于一个在所有 3 个 RGB 通道上查看一个 2×2 像素块的卷积过滤器。那问题来了：我们有什么理由去同时考虑图像区域和通道？ 在 Inception 中，我们开始将两者稍微分开。我们使用 1×1 的卷积将原始输入投射到多个分开的更小的输入空间，而且对于其中的每个输入空间，我们都使用一种不同类型的过滤器来对这些数据的更小的 3D 模块执行变换。Xception 更进一步。不再只是将输入数据分割成几个压缩的数据块，而是为每个输出通道单独映射空间相关性，然后再执行 1×1 的深度方面的卷积来获取跨通道的相关性。 其作者指出这本质上相当于一种已有的被称为「深度方面可分的卷积（depthwise separable convolution）」的运算，它包含一个深度方面的卷积（一个为每个通道单独执行的空间卷积），后面跟着一个逐点的卷积（一个跨通道的 1×1 卷积）。我们可以将其看作是首先求跨一个 2D 空间的相关性，然后再求跨一个 1D 空间的相关性。可以看出，这种 2D+1D 映射学起来比全 3D 映射更加简单。 而且这种做法是有效的！在 ImageNet 数据集上，Xception 的表现稍稍优于 Inception v3，而且在一个有 17000 类的更大规模的图像分类数据集上的表现更是好得多。最重要的是，它的模型参数的数量和 Inception 一样多，说明它的计算效率也更高。Xception 非常新（2017 年 4 月才公开），但正如前面提到的那样，这个架构已经在通过 MobileNet 助力谷歌的移动视觉应用了。 有趣的事实： Xception 的作者也是 Keras 的作者。Francois Chollet 是真正的大神。 未来发展这就是 ResNet、Inception 和 Xception！我坚信我们需要对这些网络有很好的直观理解，因为它们在研究界和产业界的应用越来越普遍。我们甚至可以通过所谓的迁移学习将它们用在我们自己的应用中。 迁移学习是一种机器学习技术，即我们可以将一个领域的知识（比如 ImageNet）应用到目标领域，从而可以极大减少所需要的数据点。在实践中，这通常涉及到使用来自 ResNet、Inception 等的预训练的权重初始化模型，然后要么将其用作特征提取器，要么就在一个新数据集上对最后几层进行微调。使用迁移学习，这些模型可以在任何我们想要执行的相关任务上得到重新利用，从自动驾驶汽车的目标检测到为视频片段生成描述。","link":"/2017/12/18/理解ResNet、Inception与Xception/"}],"tags":[{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"OPTICS","slug":"OPTICS","link":"/tags/OPTICS/"},{"name":"cluster","slug":"cluster","link":"/tags/cluster/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"RBM","slug":"RBM","link":"/tags/RBM/"},{"name":"CD","slug":"CD","link":"/tags/CD/"},{"name":"MCMC","slug":"MCMC","link":"/tags/MCMC/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Gradient descent","slug":"Gradient-descent","link":"/tags/Gradient-descent/"},{"name":"BGD","slug":"BGD","link":"/tags/BGD/"},{"name":"SGD","slug":"SGD","link":"/tags/SGD/"},{"name":"Mini-Batch","slug":"Mini-Batch","link":"/tags/Mini-Batch/"},{"name":"ResNet","slug":"ResNet","link":"/tags/ResNet/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Inception","slug":"Inception","link":"/tags/Inception/"},{"name":"Xception","slug":"Xception","link":"/tags/Xception/"}],"categories":[{"name":"Basic Tools","slug":"Basic-Tools","link":"/categories/Basic-Tools/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Deep Learing","slug":"Deep-Learing","link":"/categories/Deep-Learing/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"}]}